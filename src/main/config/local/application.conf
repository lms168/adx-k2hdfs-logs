akka{
  # 日志级别， DEBUG, INFO
  loglevel = "DEBUG"
  # do not edit this
  loggers = ["akka.event.slf4j.Slf4jLogger"]
}


main {
  name = "hello"
  http {
    host = "0.0.0.0"
    port = 7777
  }
}

akka.kafka.producer {
  # Tuning parameter of how many sends that can run in parallel.
  parallelism = 10000
  kafka-clients {
    bootstrap.servers = "adx61:9092"
    #  bootstrap.servers = "127.0.0.1:9092"
    acks = "all"
    retries = 5
    batch.size = 16384
    linger.ms = 1
    buffer.memory = 33554432
  }
}


# Properties for akka.kafka.ConsumerSettings can be
# defined in this section or a configuration section with
# the same layout.
akka.kafka.consumer {
  # Tuning property of scheduled polls.
  poll-interval = 50ms

  # Tuning property of the `KafkaConsumer.poll` parameter.
  # Note that non-zero value means that blocking of the thread that
  # is executing the stage will be blocked.
  poll-timeout = 50ms

  # The stage will be await outstanding offset commit requests before
  # shutting down, but if that takes longer than this timeout it will
  # stop forcefully.
  stop-timeout = 30s

  # How long to wait for `KafkaConsumer.close`
  close-timeout = 20s

  # If offset commit requests are not completed within this timeout
  # the returned Future is completed `TimeoutException`.
  commit-timeout = 15s

  # If the KafkaConsumer can't connect to the broker the poll will be
  # aborted after this timeout. The KafkaConsumerActor will throw
  # org.apache.kafka.common.errors.WakeupException which will be ignored
  # until max-wakeups limit gets exceeded.
  wakeup-timeout = 3s

  # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
  max-wakeups = 10

  # Fully qualified config path which holds the dispatcher configuration
  # to be used by the KafkaConsumerActor. Some blocking may occur.
  use-dispatcher = "akka.kafka.default-dispatcher"

  # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
  # can be defined in this configuration section.
  kafka-clients {
    auto.offset.reset = "latest"
    bootstrap.servers = "adx61:9092"
    # Disable auto-commit by default
    enable.auto.commit = false
  }
}

kafka.topic-out = "adx-logs-test2"

kafka.consumer {
  topics = ["adx-logs-test2", "adx-log"]
  groupId = "adx-logs-test20-group"
}

adx-logs.file {
  //已上传文件前缀
   uploadedSign = "uploaded-"
  //正点写入文件前缀
   onTimeFileSign = "0-"
  //文件5分钟没有发生变更则认为该文件可以上传了
  uploadWaitTime = 5.minutes

  original {
    rootPath = "/home/lms/adx-logs/original/"
    fileName = "adx-logs.txt"
  }
  formate{
    rootPath = "/home/lms/adx-logs/formate/"
    fileName = "adx-logs.txt"
  }

}

upload {
  //用户
  user = "root"
  //hdfs地址
  defaultFS = "hdfs://192.168.0.61:9000"
  //要上传的本地文件路径
  srcPath = ${adx-logs.file.formate.rootPath}
  //要上传的hdfs路径
  destHdfsPath = "/adx-logs/formate/"
}






//hive{
//  hive2server {
//    url = "jdbc:hive2://192.168.0.61:10000/default",
//    pwd = ""
//    user = ""
//  }
//
//  tables {
//    logs_partition = """CREATE external TABLE logs_partition(
//  `createTime` bigint,
//  `ecpm` double,
//  `pubAppAdId` bigint,
//  `pubId` bigint,
//  `pubAppId` bigint,
//  `reqId` string,
//  `adType` int,
//  `dspType` int,
//  `id` string,
//  `ip` string,
//  `longitude` double,
//  `latitude` double,
//  `machType` string,
//  `netCoonType` string,
//  `tMobile` string,
//  `imei` string,
//  `imsi` string,
//  `did` string,
//  `apiVersion` int,
//  `dspId` bigint,
//  `dspAppId` bigint,
//  `operator` string,
//  `expireTime` bigint,
//  `ua` string,
//  `uaMd5` string,
//  `calcCount` bigint,
//  `time` bigint)
//  partitioned by(y string,m string,d string,h string)
//  ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
//  stored as textfile
//  location "/adx-logs/formate" """
//  }
//}